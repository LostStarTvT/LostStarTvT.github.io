---
layout: post
title: Algorithm：海量数据处理算法
tags: java
---

> 记录常见的海量数据问题以及处理方法。

#  目录

通过hash加取模的操作能够将数据很好的归一并且进行调整。


## 微博热搜是怎么实现的？

微博的热搜问题可以简化为：1亿数据中有三百万不重复的数据，然后按照搜索的频率找到前top100的搜索的词频。那么如何实现呢？

首先需要知道，排序需要有一个依据，那么微博热搜排序的依据是什么呢？ 就是搜索的文本的次数，那么怎么统计本文的次数呢？ 如果仅仅单纯的比较文本对于很大的文本来说效率非常的低，现今大部分都是基于Hash函数进行去重，相当于一个数组<文本，出现的频率>，那么排序的依据就是出现的频率，越高的越靠前。于是，问题可以简化为统计和排序。

1.WordCount

因为微博搜索的数据都是文本，对于文本的次数排序，就需要使用到hash函数，即首先使用hash函数进行取模并进行存储，key为文本的hash值，然后value为数据出现的次数。这时候便实现了第一步的去重和统计。

2.TopN：

因为内存有限，所有可以使用堆排序算法，即首先将前100个数据进行建堆，然后依次将后面的数据入堆，当遍历完所有的数据以后，便能够找到前topk的数据，在入堆之前，首先将数据与堆顶数据比较，如果小于的话则直接下一个，如果大于的话，则入堆，重新建堆。

另外，当碰到海量数据的时候，也需要进行数据的切分，将数据分成两份使用两台机器进行统计，然后进行合并。换句话说，使用硬件换取排序的速度。

![topN.png](https://pic.tyzhang.top/images/2020/09/29/topN.png)

## 少量内存统计词频

**3、有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。**

堆排序+分治，比如说统计一个1000个词的频率，那么就将词分成10个100份的小文件，然后分别找出其中10的最小值，然后再将这些数据形成一个100个数据，找出其中最大的10个值，

方案：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为x0,x1,...x4999）中。这样每个文件大概是200k左右。

如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。
对每个小文件，统计每个文件中出现的词以及相应的频率（可以采用trie树/hash_map等），并取出出现频率最大的100个词（可以用含100个结点的最小堆），并把100个词及相应的频率存入文件，这样又得到了5000个文件。下一步就是把这5000个文件进行归并（类似与归并排序）的过程了。

## 海量数据是否存在某个数

给40亿个不重复的unsigned的整数，没有排过序，再给一个数据，如何快速判断这个数是否在这个10亿个数据中？

首先可以使用快速排序，然后使用二分查找，这样以后便可以进行对应的查找。

还有一种操作，就是将按照比特位将数据进行切分，因为无符号数据工作30位，将这些数据分为

1. 最高位0
2. 最高位1

分完以后，然后在将数据分为

1. 次高位为0
2. 次高位为1

这样在进行查找的时候只需要o(logn)便能够进行查找到。

## 海量数据找出出现频率最高的值

归结为TOP 1 问题。参考TOP的解法。

