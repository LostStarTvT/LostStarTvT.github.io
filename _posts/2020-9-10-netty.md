---
layout: post
title: Java：Netty框架
tags: java  
---


> 记录一下学习netty知识，只是涉及一些理论知识。参考《netty实战》、[netty4学习笔记](https://blog.csdn.net/zxhoo/article/details/17264263) 大佬写的通俗易懂。[理解channelPipeLine](https://www.cnblogs.com/qdhxhz/p/10234908.html)  [netty学习](https://blog.csdn.net/spiderdog/category_1800249.html)  

#  目录
* 目录
{:toc}
# 简单Demo

netty依赖：

```xml
<dependency>
    <groupId>io.netty</groupId>
    <artifactId>netty-all</artifactId>
    <version>4.1.47.Final</version>
</dependency>
```

## netty服务器

启动服务器实例。

```java
package netty.server;

public class EchoServer {
    private final  int port;

    public EchoServer(int port){
        this.port = port;
    }

public  void  start() throws Exception{

        // 定义nio数据处理的实例。使用自己定义的处理实例进行处理所有的连接。
        final  EchoServerHandler serverHandler = new EchoServerHandler();

        // 开启两个线程池进行工作。
        EventLoopGroup bossGroup = new NioEventLoopGroup();
        EventLoopGroup workerGroup = new NioEventLoopGroup();
        try{
            // 进行引导和绑定服务器。
            ServerBootstrap b = new ServerBootstrap();
            // 指定NioEventLoopGroup进行处理连接
            b.group(bossGroup,workerGroup)
                    .channel(NioServerSocketChannel.class)
                    // 设置服务器端口。
                    .localAddress(new InetSocketAddress(port))
                    // 绑定数据流处理实例。
                    .childHandler(new ChannelInitializer<SocketChannel>() {
                        protected void initChannel(SocketChannel socketChannel) throws Exception {
                            // 将自定义的数据处理实例添加进去。
                            socketChannel.pipeline().addLast(serverHandler);
                        }
                    });
            // 异步绑定服务器。 带有sync表示是异步的。
            ChannelFuture f = b.bind().sync();
            f.channel().closeFuture().sync();
        }finally {
            workerGroup.shutdownGracefully();
            bossGroup.shutdownGracefully();
        }
    }

    public static void main(String[] args) throws Exception {
        // 监听8080 端口
        int port = 8080;
        new EchoServer(port).start();
    }
}
```

netty服务器handler

```java
package netty.server;

@ChannelHandler.Sharable
public class EchoServerHandler extends ChannelInboundHandlerAdapter {

    @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception {
        ByteBuf in = (ByteBuf) msg;
        System.out.println("Server received" + in.toString(CharsetUtil.UTF_8));
    }

    /**
     *  将未决消息冲刷到远程节点。。
     * @param ctx
     * @throws Exception
     */
    @Override
    public void channelReadComplete(ChannelHandlerContext ctx) throws Exception {
        ctx.writeAndFlush(Unpooled.EMPTY_BUFFER).addListener(ChannelFutureListener.CLOSE);
    }

    /**
     * 捕获异常
     * @param ctx
     * @param cause
     * @throws Exception
     */
    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception {
        cause.printStackTrace();
        ctx.close();
    }
}

```

## netty客户端

启动客户端实例。

```java
package netty.client;

import io.netty.bootstrap.Bootstrap;
import io.netty.channel.ChannelFuture;
import io.netty.channel.ChannelInitializer;
import io.netty.channel.EventLoopGroup;
import io.netty.channel.nio.NioEventLoopGroup;
import io.netty.channel.socket.SocketChannel;
import io.netty.channel.socket.nio.NioSocketChannel;

import java.net.InetSocketAddress;

//netty 客户端。
public class EchoClient {

    private final  String host;
    private final int port;

    public EchoClient(String host,int port){
        this.host = host;
        this.port = port;
    }

    public void start() throws Exception{

        EventLoopGroup group = new NioEventLoopGroup();
        try {
            // 引导类，将group和channel、channelHandel 绑定在一起。
            Bootstrap b = new Bootstrap();

            b.group(group)
                    .channel(NioSocketChannel.class)
                    .remoteAddress(new InetSocketAddress(host,port))
                    .handler(new ChannelInitializer<SocketChannel>() {
                        @Override
                        protected void initChannel(SocketChannel socketChannel) throws Exception {
                            // 在管道中添加一个handler 进行处理数据。
                            socketChannel.pipeline().addLast(new EchoClientHandler());
                        }
                    });
            ChannelFuture f = b.connect().sync();
            f.channel().closeFuture().sync();
        }finally {
            group.shutdownGracefully();
        }
    }

    public static void main(String[] args) throws Exception {

        // 访问本地接口  然后进行访问8080 端口。
        String host = "127.0.0.1";
        int port = 8080;
        new EchoClient(host,port).start();
    }
}
```

客户端handler

```java
package netty.client;

//处理连接流
public class EchoClientHandler extends SimpleChannelInboundHandler<ByteBuf> {

    @Override
    public void channelActive(ChannelHandlerContext ctx) throws Exception {
        ctx.writeAndFlush(Unpooled.copiedBuffer("Netty rocks",CharsetUtil.UTF_8));
    }

    @Override
    protected void channelRead0(ChannelHandlerContext channelHandlerContext, ByteBuf byteBuf) throws Exception {
        System.out.println("Client received: " + byteBuf.toString(CharsetUtil.UTF_8));
    }

    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception {
        cause.printStackTrace();
        ctx.close();
    }
}
```

# NIO知识简介

对于nio来说，主要有三个重要的概念**select，channel，byteBuffer**三个抽象的概念，而netty就是对其进行的封装，另外netty是基于Reactor线程模式进行的IO多路复用的设计，首先什么叫做Reactor设计模式？其中Java中的NIO是属于同步非阻塞的IO，关于IO多路复用，Java中没有相应的IO模型，但是有相应的响应模式，Reactor就是基于NIO实现的一种多路复用的模式，具体的实现方式如下：Reactor使用两个线程池进行IO连接的处理，一个线程池解决网络的连接，一个线程池解决数据的传输，而对于数据计算来说，一般需要使用另外一个工作线程池复杂数据的计算，即不能使用Reactor线程池中的线程进行复杂的计算，这样会出现连接大量丢失的情况。而EventLoop其实也就是Reactor线程模型的实现。

## 通道 Channel

通道(Channel) 是对BIO中流的模拟，到任何目的地（或来自任何地方）的所有数据都必须通过一个通道对象。通道与流的不同之处就在与通道是双向的，流只能在一个方向上移动（一个流要么用于读，要么用于写），而通道可以用于读、写或者同时用于读写，因为通道是双向的，所以它可以比流更好的反应底层操作系统的真实情况。

## 缓冲区 ByteBuff

尽管通道是用于读写数据，但是我们却不能直接操作通道进行读写，而是通过缓冲区(Buffer)完成，缓冲区实际上是一个容器对象，发送给通道的所有对象都必须先放到缓冲区中，同样从通道中读取任何数据都要先读到缓冲区中。

缓存区体现了NIO和BIO的一个重要区别。在BIO中，读写可以直接操作流对象，简单讲，缓存区通常是一个字节数组，也可以使用其他类型的数组，但是缓冲区又不仅仅是一个数组，它提供了对数据的结构化访问，而且可以跟踪系统的读写进程。

## 选择器 Selector

Java NIO 提供来了选择器组件(Selector)，用于同时检测多个通道的事件以实现异步的IO，我们将感兴趣的事件注册到Selecor上，当事件发生时可以通过Selector获得事件发生的通道，并进行相关的操作。

异步IO的一个优势就是，它允许你同时根据大量输入、输出执行IO操作，同步IO一般要借助于轮询，或者常见许多线程以处理大量的链接，其实这也是基于线程间的通信有socket这个原因。使用异步IO你可以监听任意数量的通道事件，不必轮询，也不必启动额外线程。

## Epoll & Select

Epoll 和Select 其实都是基于观察者模式的实现，即会向socket注册监听，当事件触发以后，就会执行监听，这样就实现了异步。

其中epoll有三个主要的函数：

- epoll_create：用于在内核里直接创建事件表，它也叫事件注册函数，这也是前面提到的，它比select这种系统调用高级的地方之一是它直接在内核里创建事件表，省去了从用户空间到内核空间复制的开销。可以结合JDK NIO的I/O事件来理解，epoll将用户关心的文件描述符（fd）通通提前放到内核的一个表里，而不是像select/poll那样，每次调用都需要重复传入用户空间的fd集合到内核

- epoll_ctl：用于添加，删除或修改指定的fd及其感兴趣的I/O事件，在JDK里就是类似调用Channel的register方法，为Channel注册I/O多路复用器，以及感兴趣的I/O事件

- epoll_wait：用于等待先前指定的fd里的I/O事件就绪，如果有I/O事件就绪，那么epoll无需遍历完整的被监听的fd集合，只需要遍历那些被内核I/O事件异步唤醒而加入就绪队列的fd集合即可。而select/poll系统调用每次调用都要全部遍历一遍fd集合

相当于是当socket事件被注册进来以后，就会调用epoll_wait()方法，否则线程就是被阻塞的，因为是使用死循环进行循环执行epoll_wait()方法的。

epoll其实是一个机制，而不是一个函数，是一个IO多路复用的实现机制。epoll 其实就相当于老师监考，学生有问题进行举手，然后老师去处理事情，否则老师就等着。这样就实现了一一个老师可以服务很多的学生。

同理，select就是学生不举手，老师一边又一遍的去轮询学生有没有问题。

另外，select会不断的执行将socket不断的从用户态复制到内核态，然后从内核态复制到用户态的过程，而epoll 则是直接使用epoll_ctl方法，将socket直接保存在内核态的一个eventpoll对象上，这样就避免了来回的复制。

对于select与epoll的区别，其实可以理解为select是对数组进行遍历，而epoll是对红黑树进行查找，其中查找的key可以是使用ip+端口号作为唯一索引，因为这是一个连接的主要区别特征，另外，epoll使用eventpoll对象监听所有的socket，当某个socket数据传输完以后，便会使用中断通知数据已经传输好完成。

**就绪列表的数据结构**

就绪列表引用着就绪的socket，所以它应能够快速的插入数据。

程序可能随时调用epoll_ctl添加监视socket，也可能随时删除。当删除时，若该socket已经存放在就绪列表中，它也应该被移除。

所以就绪列表应是一种能够快速插入和删除的数据结构。双向链表就是这样一种数据结构，epoll使用双向链表来实现就绪队列（对应上图的rdllist）。

**索引结构**

既然epoll将“维护监视队列”和“进程阻塞”分离，也意味着需要有个数据结构来保存监视的socket。至少要方便的添加和移除，还要便于搜索，以避免重复添加。红黑树是一种自平衡二叉查找树，搜索、插入和删除时间复杂度都是O(log(N))，效率较好。epoll使用了红黑树作为索引结构（对应上图的rbr）。

> ps：因为操作系统要兼顾多种功能，以及由更多需要保存的数据，rdlist并非直接引用socket，而是通过epitem间接引用，红黑树的节点也是epitem对象。同样，文件系统也并非直接引用着socket。为方便理解，本文中省略了一些间接结构。

从下面可以看出，epoll方式是将socket等待队列指向evetpoll，而select是将socket的等待队列指向工作线程，但是只是在工作线程被阻塞的时候才会指向，当工作线程工作的时候又会删除socket向线程的指向，如下下图所示。所以epoll就是用一个不变的代理eventpoll进行解决重复指向，有点类似于JVM中的句柄模式，因为工作线程是会变的，所以需要有个不变的东西去记录便的东西，这样对于socket来说，就不需要频繁的更新等待队列。

![epoll.png](https://pic.tyzhang.top/images/2020/09/18/epoll.png)

下图主要包括两个部分，socket等待队列指向工作线程，工作线程处理socket的数据。

下图中的select添加等待队列，即将工作线程添加到socket的等待队列中，具体的操作也就是将socket的等待队列指针指向工作线程，发现数据没有准备好，那么就阻塞工作线程。等到有连接的数据处理好以后，因为socket执行工作线程，所以便会唤醒工作线程， 然后工作线程进行轮询是哪一个连接数据准备好，处理完数据以后，便重新将所有的socket的等待队列执行工作线程并且阻塞，重复以上的内容。

而epoll则是当socket等待队列指向工作线程以后，如果有数据传输完成，那么就直接遍历红黑树索引到该线程，进行处理数据，然后直接阻塞自己，不会重复添加等待队列删除等待队列，只会有新连接添加到红黑树中，将断开的连接从红黑树中删除这两个步骤。中间处理过程则是使用红黑树索引到该socket然后进行后面的计算和写回。也就是epoll将功能拆分。

另外epoll还会维护一个就绪队列，使用的是双向链表进行维护。 即select记录所有的socket是使用数组，而epoll使用的数据结果是红黑树，可以快速的添加删除并且查找。

![epollAndSelect.png](https://pic.tyzhang.top/images/2020/09/18/epollAndSelect.png)

# Reactor线程模型

有了epoll和select为什么还需要Reactor线程模型？主要是为了进行线程的分工，对于epoll来说，主要有两个步骤，Socket数据连接和IO数据的读写，如果仅使用一个线程完成这些事，效率会比较低。现阶段都是多核处理器，那么就可以将这些操作进行分割，不同的线程干不同的事，比如一个线程只进行处理socket的连接，一个线程只进行IO数据的读写，一个工作线程进行复杂数据的处理，为什么可以分？因为TCP需要先进行连接然后才能进行数据的传输，另外有些连接也需要进行身份的认证等等。 而网络传输会因为网络问题变得不可控有时候持续时间比较长，所以也需要一个线程去专门的处理IO数据的读写操作，当接收到网络数据流以后，如果数据计算比较耗时，也需要一个专门的本地线程去进行复杂的计算操作，防止IO线程被阻塞影响其他连接的数据读写。那么既然可以分成这三种类型，那么如何协调这三种线程进行工作呢？Reactor线程模型就是解决这个问题，在netty和Redis中都使用了Reactor线程模型进行IO线程间的调度。

## 单线程模型

即上面最原始的所有的IO操作都是由一个线程进行处理，这也是符合人最基本的处理模型，其结构图如下：

![ReactorS.png](https://pic.tyzhang.top/images/2020/09/10/ReactorS.png)

这种只是上层的数据处理分发，因为底层是使用异步非阻塞IO进行连接的处理，所以理论上单线程能够处理所有的连接，并且所有的IO操作都不会导致阻塞，其处理逻辑如下：通过Accepor类接受客户端的TCP连接请求信息，当链路建立成功之后，通过Dispatch将对应的ByteBuffer派发到指定的Handler上，进行消息解码，用户线程消息编码后通过NIO线程将消息发送给客户端。

## Reactor多线程模式

与单线程模式相比，主要多添加了一组NIO线程进行处理IO操作。

![ReactorSP.png](https://pic.tyzhang.top/images/2020/09/10/ReactorSP.png)

其特点如下：

- 有一个专门的NIO线程-Acceptor线程用于监听服务器端，接受客户端的TCP连接请求。
- 网络IO操作-读、写等由一个NIO线程池负责，线程池可以采用标准的JDK线程池实现，它包含一个任务队列和N个可用的线程，由这些NIO线程负责消息的读取、解码、编码和发送。
- 一个NIO线程可以同时处理N条链路，但是一个链路只对应一个NIO线程，防止并发操作问题。

## Reactor主从多线程模式

相当于接受者也是一个线程池。Acceptor接收到客户端TCP连接请求并处理完成后（可能包含接入认证等），将新创建的SocketChannel的读写和编码工作。但是Acceptor线程池仅仅用于客户端的登录、握手和安全认证，一旦链路建立成功，就将链路注册到后端subReactor线程池的IO线程上，由IO线程负责后续的IO操作。其中值得说明的是，在IO Pool中，每一个Channel分配到一个线程以后，就只会由这个线程进行负责，避免了多线下的并发资源竞争问题。其中主从Reactor线程模型如下所示：

![ReactorMS.png](https://pic.tyzhang.top/images/2020/09/10/ReactorMS.png)

## IO连接、IO数据接收、复杂数据计算建议

对于Channel的连接、数据处理和复杂计算业务的处理建立。

- 创建两个NioEventLoopGroup，用于逻辑隔离NIO Acceptor 和NIO I/O线程。
- 尽量不要在ChannelHandler中启动用户线程(解码后用于将POJO消息派发到后端业务线程的除外)
- 解码要放在NIO线程调用的解码Handler中进行，不要切换到用户线程中完成消息的解码
- 如果业务逻辑操作非常简单，没有复杂的业务逻辑计算，没有可能会导致线程被阻塞的磁盘操作、数据库操作、网络操作等，可以直接在NIO线程中完成业务逻辑编排，不需要切换到用户线程。
- 如果业务逻辑处理复杂，不要再NIO线程上完成，建议将解码后的POJO消息封装成Task，派发到业务线程池中由业务线程处理，以保证NIO线程尽快的被释放，处理其他的IO操作。

其中一个很重要的点就是，一般需要开启三个线程池，一个完成连接的建立，一个完成连接数据的处理，一个完成复杂业务的计算，又因为线程之间的任务切换比较费时，所以也需要综合的考虑。

为什么观察者模式可以实现注册接口，观察者模式相当于将自己想要做的事情一股脑的都交给了主题执行线程去处理，调用自己的方法可以直接的返回，然后当主题线程处理好数据以后，会直接执行观察者注入进来的逻辑代码，因为其原始的对象变量都还在内存空间，所以可以直接的操作当时的对象，所以就和观察者自己执行的方式是一样的。但是其实已经不是观察者线程进行执行，而是调用主题的线程帮忙执行，对于netty来说，就是背后业务线程池完成了处理逻辑，又因为传递进来的task里面的需要操作的对象还在内存中，所以业务线程池还能调用对象进行后续的操作，比如将数据出栈返回客户端。

## 无锁化的操作

在IO线程内部进行串行话操作，避免多线程竞争导致性能的下降问题，其实redis也就是使用了Reactor的单线程模式。表面上看串行化设计似乎CPU利用率不高，并发程度不够，但是通过调整IO线程池的线程参数，可以同时启动多个串行化的线程并行运行，这种局部无锁化的串行线程池设计相比于一个队列---多个工作线程的模型性能更优。

以上说了这么多，其实主要是在为介绍netty中的EvenLoopGroup在进行打基础。

# 为什么使用Netty？

- API简单，开发门槛低。
- 功能强大，预制了多种编解码功能，支持多种主流协议。
- 定制能力强，可以通过ChannelHandler对通信框架进行灵活的扩展
- 性能高，通过与其他业界主流的NIO框架对比，Netty的综合性能最优
- 成熟、稳定，Netty修复了已经发现的所有JDK、NIO Bug
- 社区活跃，版本迭代周期短，发现bug可以即使的被修复，同时更多的新功能会加入。
- 经历了大规模的商业应用考验，质量得到了验证，在互联网、大数据、网络游戏、企业应用、电信软件等众多行业得到了成功商用，证明了它已经完全能够满足不同行业的商业应用了。

# Netty API与TCP连接对应关系

![TCPandNetty.png](https://pic.tyzhang.top/images/2020/09/18/TCPandNetty.png)

可以发现主要有连接、数据的传输，然后其API也是一一对应的。

# Channel

对于JDK中的NIO，他有ServerSocketChannel和SocketChannel，Netty分别对其进行了封装，服务端Channel——NioServerSocketChannel，客户端Channel——NioSocketChannel，二者是一一对应的。

## ChannelHandler 

netty中使用Channel类进行封装Java NIO中的channel，所以说具体的channel操作，最终还是使用底层的Java channel进行实现，对于一个Channel来说，这时候值得是已经完成IO数据传输的Channel，也就说获取到Channel以后便可以注册多个ChannelHandler进行处理数据流，其中ChannelHandler 采用了一种叫做责任链的模式进行处理数据，即Channel中的数据会经过所有的ChannelHandler，而且ChannelHandler支持可插拔操作。

Netty中主要有两种Channel，一种是服务器的NioServerSocketChannel

![NioServerSockertChannel.png](https://pic.tyzhang.top/images/2020/09/18/NioServerSockertChannel.png)

还有就是客户端连接的NioSocketChannel

![NioSocketChannel.png](https://pic.tyzhang.top/images/2020/09/18/NioSocketChannel.png)

可以发现二者其实是有重复的接口，而顶层的接口就是Channel接口。

![Channel.png](https://pic.tyzhang.top/images/2020/09/18/Channel.png)

从顶到下，基本脉络是：Channel接口>AbstractChannel（所有Channel的骨架实现）>AbstractNioChannel（NIO模型下Channel的骨架），而从AbstractNioChannel又开始分支，

[Netty如何封装Socket客户端Channel，Netty的Channel都有哪些类型？](https://mp.weixin.qq.com/s?__biz=MzU1NjY0NzI3OQ==&mid=2247484561&idx=1&sn=e10417fc5c31bfb257e40ab52830ff99&chksm=fbc09291ccb71b87be2d54536e00f7570e66a582a08f24a35d4b7ac7ac87d6dc7f0e6dd7f301&scene=21#wechat_redirect)

总的来说，NioSocketChannel的构造器主要做了两件事：

1、调用一系列父类构造器，对SocketChannel做初始化工作

- 设置SocketChannel为非阻塞模式
- 保存初始化SocketChannel需要注册的I/O事件——OP_READ=16
- 创建SocketChannel的唯一id
- 创建SocketChannel的unsafe实现类——NioByteUnsafe，后续专门总结Netty的unsafe，这里先知道。
- **创建SocketChannel的默认pipeline组件，这个组件是Netty额外封装JDK的Channel的核心原因，目的就是更好的设计自己的架构，这里和服务端封装ServerSocketChannel是一样的逻辑，复用了代码**

2、调用配置类：主要是禁止TCP Nagle算法

前面说过每个Netty的Channel都有一个Unsafe接口与之绑定，Unsafe接口的相关实现类负责实现Netty的Channel所有I/O操作，一共有两类unsafe的实现类，即服务端Channel的NioMessageUnsafe，它的主要作用就是读新连接，还有客户端Channel的NioByteUnsafe，它的主要作用是读、写已有连接上的数据。

![ChannelUnsafe.png](https://pic.tyzhang.top/images/2020/09/18/ChannelUnsafe.png)

最后，还能知道每个Netty的Channel都有一个config——配置工具类，存储了每类Channel的底层网络配置，其继承关系如下：

![ChannelConfig.png](https://pic.tyzhang.top/images/2020/09/18/ChannelConfig.png)

## ChannelPipeLine

ChannelPipLine是线程安全的，并且是ChannelHandler的容器，类似于一种Map的结构，使用的是责任链的设计思想，即消息会在ChannelHandler中进行依次传递。对于Channelhandler而言，并不是线程安全的。因为每个Channel都会有一个ChannelPipeLine对象，但是ChannelHandler并不是线程安全，因为会有很多的ChannelPipeLine指向Channelhandler，他是共享的，处理的逻辑也就是，每个连接都会有自己私有的ChannelPipeLine，然后ChannelPipeLine指向共享的ChannelHander，所有可以通过更改ChannelPipeline，链表的指向，为每一条Channel定制不同的处理规则。

1、tail节点只处理inbound事件，即只传播被动发起的入站事件，一般是I/O线程发起的，比如内部异常传播，业务消息未处理的告警事件传播，有可读消息事件传播等其它收尾操作

2、head节点则只处理outbound事件，即只传播用户线程主动发起的事件，比如传播读、写，端口绑定，主动连接等事件，并委托Netty Channel的内部类Unsafe操作。看HeadContext类，如下，在该类确实聚合了一个unsafe属性，在TailContext类里就没有。

与自己主观的记忆是相反的，但是因为是双向链表其实头尾其实是无所谓的，



## Unsafe类

Netty服务端Channel对应的unsafe实现类是NioMessageUnsafe，而客户端Channel对应的unsafe实现类是NioByteUnsafe，一个是消息，一个是字节，通过命名就能快速的区分出：

1、服务端Channel执行Socket的I/O操作靠的是NioMessageUnsafe

2、客户端Channel执行Socket的I/O操作靠的是NioByteUnsafe

Unsafe其实是对java NIO底层接口的封装，避免用户操作这些复杂的东西。

![ChannelUnsafe2.png](https://pic.tyzhang.top/images/2020/09/18/ChannelUnsafe2.png)

为什么会出现粘包？因为TCP协议是一个面向流的协议，本身不保证业务消息的界限，其上层应用必须对这些网络字节流进行有意义的识别，否则在应用层很容易就会出现所谓的粘包现象，导致接收的数据不正确，当然这是用户的锅，不是TCP的问题。

入栈操作：这里提前接触Netty的入站事件和出站事件的概念，所谓入站事件——即inbound事件，即Netty的NIO线程主动发起的，是面向用户业务handler的操作，即都是被动发起的事件，通过fireXXX方法传播。

```java
public interface ChannelInboundHandler extends ChannelHandler {
    void channelRegistered(ChannelHandlerContext var1) throws Exception;

    void channelUnregistered(ChannelHandlerContext var1) throws Exception;

    void channelActive(ChannelHandlerContext var1) throws Exception;

    void channelInactive(ChannelHandlerContext var1) throws Exception;

    void channelRead(ChannelHandlerContext var1, Object var2) throws Exception;

    void channelReadComplete(ChannelHandlerContext var1) throws Exception;

    void userEventTriggered(ChannelHandlerContext var1, Object var2) throws Exception;

    void channelWritabilityChanged(ChannelHandlerContext var1) throws Exception;

    void exceptionCaught(ChannelHandlerContext var1, Throwable var2) throws Exception;
}
```

出栈操作：比如Channel连接成功，Channel关闭，Channel有数据可读，Channel上注册I/O多路复用器成功，Channel解除I/O多路复用器的注册，异常抛出等，这些都是被动执行的回调事件，它们的处理有专门的handler实现，统一叫入站handler。反之还有出站事件和出站handler，出站事件——即outbound事件，都是用户线程或者用户代码主动发起的事件，如下是出站事件：

```java
public interface ChannelOutboundHandler extends ChannelHandler {
    void bind(ChannelHandlerContext var1, SocketAddress var2, ChannelPromise var3) throws Exception;

    void connect(ChannelHandlerContext var1, SocketAddress var2, SocketAddress var3, ChannelPromise var4) throws Exception;

    void disconnect(ChannelHandlerContext var1, ChannelPromise var2) throws Exception;

    void close(ChannelHandlerContext var1, ChannelPromise var2) throws Exception;

    void deregister(ChannelHandlerContext var1, ChannelPromise var2) throws Exception;

    void read(ChannelHandlerContext var1) throws Exception;

    void write(ChannelHandlerContext var1, Object var2, ChannelPromise var3) throws Exception;

    void flush(ChannelHandlerContext var1) throws Exception;
}
```

言归正传，据需分析服务器读取新连接的过程，现在分析的是新练级接入，故只看入栈handler，先知道入栈事件流动的顺序是从pipeline的头节点开始，途径各个入栈handler节点，一直流动到尾部节点结束，这里就是

**Head-> Handler->ServerBootstrapAcceptor->Tail**

这得知道tail节点本质上是一个入栈handler，head节点本质是一个出栈handler

# ByteBuf



# Selector

多路复用器Selector，其实就是封装了底层NIO的select和epoll，对于select来说，就是一直轮训所有的连接有没有将数据传输好，对于epoll来说，就是一直在轮训epoll中的就绪列表中的数据。

Selector在操作系统层面使用了两个事件表，还记得之前分析Selector的源码时提到的poll数组么，这是一个注册表，当你调用channel.register时，会有一个新的元素塞入到poll数组，这个元素在JVM就是对应了selectionkey，该表不会自己清理，必须用户手动清理，对应到JDK层面就是使用key的cancel方法取消它。还有一个就绪事件表，保存已经就绪的key，两个表的转移，就是通过Selector的select调用，即Selector检测poll数组后，将里面有I/O事件就绪的Channel的fd复制到就绪事件表，对应到JDK层面就是publicSelectedKeys，而这是一个HashSet集合，它在JDK里被包装为了一个只能remove的对象，如果每轮检测后，你处理但是不remove掉对应的key，那么下次检测时，还会被Selector感知到这个key，对应到代码就是selector.selectedkeys()返回的集合里仍然包括了上次遗留的key。所以，NIO编程的一个套路是必须在处理完一个key后，remove它，如果确实有连续处理某个Channel的剩余数据的需求，那么可以remove后，等下次检测到被触发，再拿出来处理，因为JDK选择了LT工作模式。你不用担心下次检测不出来。一句话，一轮检测后，把key全部remove就完事儿了。

1、Selector的select到底返回的是什么？

本质是selectionKey集合的元素个数

它是Java NIO编程的基础，多路复用器提供了选择已经就绪的任务的能力。简单的来说，Selector会不断的轮询注册在其上的Channel，如果某个Channel上面有新的TCP连接接入、读和写事件，这个Channel就处于就绪状态，会被Selector轮询出来，然后通过SelectionKey就可以获取到就绪的Channel的集合，然后进行后续的IO操作。一个多路复用器Selector可以同时轮询多个Channel，由于JDK使用了epoll()代替传统的select实现，所有它并没有最大连接句柄1024/2048的限制，这样就意味着只需要一个线程负责Selector的轮询，就可以接入成千上万的客户端，这确实是个非常巨大的进步。

为什么底层使用epoll和select都会有Selector？ 因为selector是轮询到数据处理好的socket连接，对于select来说，那么就是轮询全部的socket，对于epoll来说，那就是轮询epoll中的就绪队列中的socket。

对于如果很多线程同时只有几个socket就绪的服务器情况，这时候使用epoll比较好，但是如果并发连接下，很多socket同时就绪，这种其实使用select和epoll是差不多的。因为此时epoll也会轮询就绪队列，如果就绪队列占大头那么其实和select是没有关系的。

select会对一直监听注册的事件，当注册的感兴趣事件被触发以后，就会被触发，相当于是监视器模式方法被调用。可以监听的事件有四种

1. 可读事件 OP_READ 
2. 可写事件 OP_WRITE
3. 连接事件 OP_CONNECT
4. 接受事件 OP_ACCEPT

一般的，服务端Channel主要就是注册OP_ACCEPT事件，客户端Channel主要注册OP_READ事件，OP_CONNECT事件，和OP_WRITE事件等。

在往底层看，TCP/IP协议栈只有两种I/O事件，就是读和写。因此JDK中的OP_ACCEPT和OP_CONNECT事件都是在读、写事件的基础上Java自己引入的，是Java自己的东西，本质上：

1. OP_ACCEPT等价于OP_READ，都属于操作系统底层的I/O读事件
2. OP_CONNECT等价于OP_WRITE，都属于操作系统底层的I/O写事件

# EventLoopGroup

这个也就是Reactor线程模型的实现，对于Netty来说一般都是推荐使用主从Reactor模型，对应到netty中，就是在bootstrap中传递进去两个EventLoopGroup，worker和boss，即boss线程池负责处理连接（Accept Pool），其中连接包括认证等操作，worker线程池负责处理IO操作(IO Pool)，比如读和写，其中值得说明的是，在IO Pool中，每一个Channel分配到一个线程以后，就只会由这个线程进行负责，避免了多线下的并发资源竞争问题。

关于worker和boss线程，为什么可以使用两个线程维护大量的channel连接？ 其实也不是只有两个线程，而是有两个工作线程池去处理。

主要就是使用Select这个抽象类，**即boss先接受连接，然后boss将连接注册到worker的select中，使用worker的select进行维护所有的连接**，而底层实现的其实也是基于select或者是epoll维护所有的连接，只不过一个数据结果是数组维护连接，一个是红黑树维护连接，但是都可以抽象为select概念，即选择器。

另外也不一定只有一个线程维护连接，其实都可以开启线程池进行处理，这样就会提高并发量。

然后还可以问问对于NioEventLoopGroup的理解，可以理解为这是一个线程池，然后调用他的next()方法便可以生成一个NioEventLoop,这个东西更像是一个只有一个线程的线程池，对于Channel来说，也需要将自己注册到NioEventLoop对象中去， 其实也就是将JDK中的channel注册到NioEventLoop中，方便处理整个的连接。

**以下是netty进行数据连接的过程，可以发现，Netty默认使用的Reactor是多线程Reactor模型。**boss做的主要操作就是接受连接后调用worker线程池的next()方法，将新的连接注册到worker的Selector中，并且一直与该线程绑定，避免多线程资源竞争。

默认情况下netty的Boss NIoEvenLoopGroup只会有一个线程去处理新的网络连接，而对于Work 会有多个线程处理IO数据的读取，默认是CPU核心数的2倍。

简单说，Netty服务端创建的boss和worker就是两个线程池，对于一个服务器的端口，bossGroup里只会启动一个NIO线程用来处理该端口上的客户端新连接的检测和接入流程。

具体的说，Netty会在服务端的Channel的pipeline上，默认创建一个新连接接入的handler，只用于服务端接入客户端新连接，而workerGroup里有多个NIO线程（默认2倍的CPU核数个），负责已建立的Channel上的读写事件的检测、注册或者处理，等操作。当boss线程池的那一个NIO线程检测到新连接后就可以稍做休息（或者继续检测处理新连接），此时worker线程池就开始忙碌，如下图所示：

![Reactor.png](https://pic.tyzhang.top/images/2020/09/18/Reactor.png)

# BootStrap



# 全异步框架

一般来说netty在进行写回数据的时候调用的方法：

```java
// 启动 RPC 服务器 这是非阻塞的，其实是注册了一个接口回调，不会阻塞调用的线程。
ChannelFuture future = bootstrap.bind(ip, port).sync();
```

首先来说，Netty的全异步模型本质就是观察者模式。  [Netty为何重复设计JDK的Future接口？](https://mp.weixin.qq.com/s?__biz=MzU1NjY0NzI3OQ==&mid=2247484352&idx=1&sn=fcb13e1f04992972b2d32d29dc81b77b&chksm=fbc095c0ccb71cd6feac3b4617ead75fc9281a91b05eae93d15f46ad0ee92095e912b9e198bc)

Netty 是一个全异步的框架，其中对于IO的连接与数据传输是使用异步传输，那么对于复杂的计算IO数据的任务，Netty也可以使用Future进行实现。

对于Future来说，提交Executor.submit()是不阻塞的，但future.get()方法是阻塞的，会等到线程结果返回，他这种设计模式其实就是希望线程隔一段时间在去获取，如果已经执行完了获取会直接的返回，如果还没有执行完，则会阻塞线程等待，所以说可以使用一个休眠的方法，或者是在执行完核心业务以后再去获取，但是有个问题，开了这么多的线程，那么也就说需要多加几个cpu才可以，这样就可以重复的利用cpu的性能。[Future机制介绍](https://zhuanlan.zhihu.com/p/54459770)FutureTask是Future的具体实现。但是如果是线程池的话可以直接的返回FutureTask。

*所以一般需要使用future.isDone先判断任务是否全部执行完成，完成后再使用future.get得到结果*

Future常用的编程模式。

三段式的编程：1.启动多线程任务2.处理其他事3.收集多线程任务结果，Future虽然可以实现获取异步执行结果的需求，但是它没有提供通知的机制，要么使用阻塞，在future.get()的地方等待future返回的结果，这时又变成同步操作；要么使用isDone()轮询地判断Future是否完成，这样会耗费CPU的资源。
解决方法：[CompletionService](https://www.jianshu.com/p/c4a31f914cc7)和CompletableFuture（**按照任务完成的先后顺序获取任务的结果**）因为完成的任务会按照顺序放到一个Queue中，然后取去结果的时候也是按照先完成的先出来， 

[Netty源码阅读的思考---耗时业务到底该如何处理](https://www.jianshu.com/p/727bbc7454dc)

不能再Netty中做耗时的操作，不可预料的操作，比如数据库，网络请求等，这样会严重影响Netty对Socket的处理速度，其一个解决方法就是将耗时任务添加到异步线程池中。

[Netty耗时的业务逻辑应该写在哪儿，有什么注意事项？](https://www.cnblogs.com/kubixuesheng/p/12641418.html) 这篇文章完美解决我得困惑，即使用业务线程池的方式进行执行业务的逻辑。

Netty建议使用它自身提供的业务线程池来驱动非I/O的耗时业务逻辑，如果业务逻辑执行时间很短或者是完全异步的，那么不需要使用额外的非I/O线程池。而且具体用法是Netty在添加handler时，在ChannelPipeline接口提供了一个重载的addLast方法，专用于为对应handler添加Netty业务线程池，如下：

对于一个耗时的业务逻辑，netty提供了可以提交业务线程池的ChannelHandler的机制，其实也就是说有两种方式，一种是自己添加线程池，然后将耗时业务添加进去，另外一种就是使用netty自带的处理线程池handler。

```java
public static void main(String[] args) {

    EventLoopGroup boseGroup = new NioEventLoopGroup();
    EventLoopGroup workerGroup = new NioEventLoopGroup(); // 负责处理的IO线程
    // 工作线程
    final EventExecutorGroup businessGroup = new DefaultEventExecutorGroup(16);

    ServerBootstrap serverBootstrap = new ServerBootstrap();
    serverBootstrap.group(boseGroup, workerGroup)
        .channel(NioServerSocketChannel.class) // 配置非阻塞IO
        .childOption(ChannelOption.SO_KEEPALIVE,true) //接受长连接
        .childOption(ChannelOption.TCP_NODELAY,true) // 关闭TCP协议的nagle算法
        .option(ChannelOption.SO_BACKLOG,100) // 配置等待队列的最大长度
        .childAttr(AttributeKey.newInstance("clientKey"),"clientValue") // 配置客户端属性
        .attr(AttributeKey.newInstance("serverName"),"nettyServer") // 配置服务器端属性
        .childHandler(new ChannelInitializer<SocketChannel>() {
            @Override
            protected void initChannel(SocketChannel ch) throws Exception {
                ChannelPipeline p = ch.pipeline();
                p.addLast(businessGroup,new BusinessHandler()); // 耗时任务添加到任务线程中。
                p.addLast(new EchoClientHandler()); // 新连接处理器。
            }
        });
}
```

然后在Channel中进行调用：

```java
public class BusinessHandler extends ChannelInboundHandlerAdapter {
    @Override
    public void channelRegistered(ChannelHandlerContext ctx) throws Exception {
        super.channelRegistered(ctx);
    }

    @Override
    public void channelActive(ChannelHandlerContext ctx) throws Exception {
        super.channelActive(ctx);
    }

    @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception {
        // 使用业务线程执行耗时任务。
        super.channelRead(ctx, msg);
        Thread.sleep(3000);
        super.channelRead(ctx,msg);
    }
}
```

最后打印日志显示的：

![nettyLog.png](https://pic.tyzhang.top/images/2020/09/18/nettyLog.png)

可以看出来耗时的任务被自动的添加到了bussinessGroup，然后在执行完任务以后，因为调用了super.channelRead()方法，处理的结果还会被传递到后面的handler中，可以返回通过这种方式不会阻塞NIO线程，即worker线程，完美的解决耗时问题。

## 异步的执行结果怎么回到Netty的NIO线程？

```java
@Override
public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception {
    // 使用业务线程执行耗时任务。
    super.channelRead(ctx, msg);
    Thread.sleep(3000);
    super.channelRead(ctx,msg);
}
```

该方法是一个回调的用户事件，当Channel里有数据可读时，Netty会主动调用它，这种机制后续专题总结，这里知道结论即可。其中`super.channelRead(ctx,msg);`——会将该channelRead事件继续传播给下一个handler节点，即执行到下一个入站处理器——EchoServerHandler的channelRead方法。而在pipeline上传播这个事件时，Netty会对其驱动的传播过程做一个判断。看如下的invokeChannelRead方法源码：其中参数next是入站节点EchoServerHandler，其executor是NIO线程，核心代码如下红框处——会做一个判断：

```java
static void invokeChannelRead(final AbstractChannelHandlerContext next, Object msg) {
    final Object m = next.pipeline.touch(ObjectUtil.checkNotNull(msg, "msg"), next);
    EventExecutor executor = next.executor();
    // 判断是否是NIO线程。
    if (executor.inEventLoop()) {
        next.invokeChannelRead(m);
    } else {
        executor.execute(new Runnable() {
            @Override
            public void run() {
                next.invokeChannelRead(m);
            }
        });
    }
}
```

如果当前执行的线程是Netty的NIO线程（就是该Channel绑定的那个NIO线程，即executor，暂时不理解也没关系，知道结论，后续专题分析），那么就直接驱动，如果不是NIO线程，那么会将该流程封装成一个新的task扔到NIO线程的MPSCQ，排队等待被NIO线程处理,因此将耗时的业务逻辑放到非NIO线程池处理，也不会影响Netty的I/O调度，仍然能通过NIO线程向客户端返回结果。

## 耗时任务的拒绝策略:

JDK线程池默认是AbortPolicy，即主动抛出RejectedExecutionException

回到Netty，如果使用其非I/O线程池不当，可能造成NIO线程阻塞，比如业务上有人会设置线程池满的拒绝策略为CallerRunsPolicy 策略，这导致会由调用方的线程——NioEventLoop线程执行业务逻辑，最终导致NioEventLoop线程可能被长时间阻塞，在服务端就是无法及时的读取新的请求消息。

实际使用Netty时，一定注意这个坑。即当提交的任务的阻塞队列满时，再向队列加入新的任务，千万不能阻塞NIO线程，要么丢弃当前任务，或者使用流控并向业务方和运维人员报警的方式规避这个问题，比如及时的动态扩容，或者提高算法能力，提升机器性能等。

# FutureChannel

Netty中的Future是继承了JUC中的Future，然后在juc的基础上增加了回调监听，因为原生的Future只能通过不断的查询任务是否已经做好，然后在去获取，这种是同步非阻塞的方式，而Netty为了实现全异步，在其基础上增加了注册接口回调的方法，

原生Future， 需要说明的是，接口回调其实也就是观察者模式的一种实现，只不过接口回调是一对一的观察者，而观察者模式可以是一对多，即一个主题多个观察则监听。

```java
public interface Future<V> {
    boolean cancel(boolean var1);

    boolean isCancelled();

    boolean isDone();

    V get() throws InterruptedException, ExecutionException;

    V get(long var1, TimeUnit var3) throws InterruptedException, ExecutionException, TimeoutException;
}
```

nettyFuture

```java

public interface Future<V> extends java.util.concurrent.Future<V> {
    boolean isSuccess();

    boolean isCancellable();

    Throwable cause();
	
    // 增加了注册接口回调的方法，其实也就是注册观察者。
    Future<V> addListener(GenericFutureListener<? extends Future<? super V>> var1);

    Future<V> addListeners(GenericFutureListener<? extends Future<? super V>>... var1);

    Future<V> removeListener(GenericFutureListener<? extends Future<? super V>> var1);

    Future<V> removeListeners(GenericFutureListener<? extends Future<? super V>>... var1);

    Future<V> sync() throws InterruptedException;

    Future<V> syncUninterruptibly();

    Future<V> await() throws InterruptedException;

    Future<V> awaitUninterruptibly();

    boolean await(long var1, TimeUnit var3) throws InterruptedException;

    boolean await(long var1) throws InterruptedException;

    boolean awaitUninterruptibly(long var1, TimeUnit var3);

    boolean awaitUninterruptibly(long var1);

    V getNow();

    boolean cancel(boolean var1);
}

```



# API解析

# Channel解析

从以上的代码可以看出，在新建一个Netty的时候，主要的就是自定义的NettyHandler，顾名思义，Handler也就是处理器，处理数据在管道中的数据。另外，一个Channel可以有很多的Handler的，Netty也是通过这种方式实现Netty的高扩展性，即通过增加新的Handler进而实现更多的功能。其中Channel中最重要的几个类为:`ChannelHandlerContext`、`ChannelPipeline`、`ChannelInboundHandler`、`ChannelOutboundHandler`。整个的Channel就是通过这几个类进行控制的。

## ChannelHandler

主要有两种Handler，`ChannelInboundHandler`、`ChannelOutboundHandler`分别处理进管道和出管道。对于一个服务器来说，In表示外部数据进入到服务，out表示数据出服务器。对于客户端来说也是如此。从上面可以发现，我们只需要各自实现客户端和服务器的ChannelInboundHandler即可，而不需要实现outHandler，主要是因为outHandler有一个默认的处理器。即对于出去的数据进行记性相应的处理。其实只需要记住，对于服务器和客户端来说，数据在进入或者流出的时候，首先会被inHandler进行处理，然后被outHandler进行处理，最后发送给对方。  

比如说服务器，在进行调用` ChannelFuture f = b.connect().sync();`时，最后调用的就是ChannelOutboundhandler进行发送出去，因为Handler是链式存储，最后一个OutHandler负责把数据丢出去。

**handler的执行顺序**  

```java
ch.pipeline().addLast(new OutboundHandler1());
ch.pipeline().addLast(new OutboundHandler2());
ch.pipeline().addLast(new InboundHandler1());
ch.pipeline().addLast(new InboundHandler2());
```

执行的顺序为:

```shell
InboundHandler1--> InboundHandler2 -->OutboundHandler2 -->OutboundHandler1
```

即入站正序，出站逆序。  

## ChannelPipeline

对于一个抽象的Channel来说，每一个连接都会生成一个Channel，对应着也会生成一个pipeline，对于ChannelPipeline来说，其实就是ChannelHandler实例对象的链表，用于处理或者截获通道的接收和发送数据。它提供了一种高级的截取过滤模式。让用户可以在ChannelPipeline中完全控制一个时间以及如何处理ChannelHandler与ChannelPipeLine的交互。

![pipeline.png](https://pic.tyzhang.top/images/2020/06/22/pipeline.png)

常用方法：

```java
ChannelPipeline pipeline = ch.pipeline();	
	addFirst(...)   //添加ChannelHandler在ChannelPipeline的第一个位置
    addBefore(...)   //在ChannelPipeline中指定的ChannelHandler名称之前添加ChannelHandler
    addAfter(...)   //在ChannelPipeline中指定的ChannelHandler名称之后添加ChannelHandler
    addLast(...)   //在ChannelPipeline的末尾添加ChannelHandler
    remove(...)   //删除ChannelPipeline中指定的ChannelHandler
    replace(...)   //替换ChannelPipeline中指定的ChannelHandler
```

所以说，因为我们主要需要定义Handler，所有只需调用pipeLine就可以实现对应的功能。

## ChannelHandlerContext

其中ChannelPipline并不直接的操作ChannelHandler，而是通过Context进行处理Handler。类似于ChannelHandlerContext为pipeLine的节点，使用ChannelHandlerContext完成链表的连接和插入删除。而ChannelHandlerContext中的节点又是Handler（即是handler的封装类。），所以可以通过ChannelHandlerContext来调用Handler对应的接口方法，从而实现对应的操作。

```java
/**
  * 可以看到,DefaultChinnelPipeline 内部使用了两个特殊的Hander 来表示Handel链的头和尾。
  */
public DefaultChannelPipeline(AbstractChannel channel) {
    if (channel == null) {
        throw new NullPointerException("channel");
    }
    this.channel = channel;

    TailHandler tailHandler = new TailHandler();
    tail = new DefaultChannelHandlerContext(this, null, generateName(tailHandler), tailHandler);

    HeadHandler headHandler = new HeadHandler(channel.unsafe());
    head = new DefaultChannelHandlerContext(this, null, generateName(headHandler), headHandler);

    head.next = tail;
    tail.prev = head;
}
```

所以对于DefaultChinnelPipeline它的Handel头部和尾部的Handel是固定的,我们所添加的Handel是添加在这个头和尾之前的Handel。

一些具体的细节可以看相应的博客。

# BootStrap解析

[参考连接](https://blog.csdn.net/zxhoo/article/details/17419229)  

我们在进行构建netty的时候使用的是builder模式，主要就是通过这个引导类将所有的组件连接到一起，然后进行调用。整个的组装方式如下：

![bootStrap.png](https://pic.tyzhang.top/images/2020/06/22/bootStrap.png)

最终实现的就是往EvenLoopGroup中添加了一个Channel，其中Channel中的Handler已经定义好。然后就可以接客了。其中EvenLoop也即是继承java的线程，然后进行监控外部连接的。

# EventLoop解析

对于EvenLoopGroup来说，就是管理了好多的EvenLoop员工进行处理连接，其中一个EvenLoop可以处理很多的Channel，多个人就可以处理更多的。

另外，对于服务器来说，一种常用的方式就是构造两个EventLoop来处理连接。

```java
EvenLoopGroup bossGroup = new NioEventLoopGroup();
EvenLoopGroup workGroup = new NioEventLoopGroup();
```

这种构造方式就是bossGroup来进行处理新来的连接，比如判断一个新来的连接要不要接受，而workGroup则是处理被bossGroup确定接受的连接，也就是处理分开。这种有点分类的感觉。使得处理的效率更高。

还有很多的东西没有写，还是需要多看那个系列的博客， 然后进行分析实战，比如说netty还支持编码器，这样也需要进行处理。未完待续